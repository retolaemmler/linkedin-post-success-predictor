{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4acb1290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/rlaemmler/Downloads/Transferarbeit/.venv/lib/python3.14/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/rlaemmler/Downloads/Transferarbeit/.venv/lib/python3.14/site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rlaemmler/Downloads/Transferarbeit/.venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rlaemmler/Downloads/Transferarbeit/.venv/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rlaemmler/Downloads/Transferarbeit/.venv/lib/python3.14/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rlaemmler/Downloads/Transferarbeit/.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f833212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model - Predicting High Performing Content\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your data folder\n",
    "INPUT_FILE = \"../data/Raw_Dataset_LinkedIn.csv\"\n",
    "OUTPUT_FILE = \"../data/Master_Dataset_LinkedIn.csv\"\n",
    "\n",
    "#Configuration constants\n",
    "MAX_POSTS_PER_USER = 350\n",
    "MAX_RELATIVE_ENGAGEMENT = 150\n",
    "MIN_POST_LENGTH = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f33f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander Rüegg : 2\n",
      "Andreas Stutz : 114\n",
      "Andy Lavicka : 332\n",
      "Arinda Huber-Bouman : 120\n",
      "Beat Brun : 106\n",
      "Bernardo Romero : 6\n",
      "Bernhard von Allmen : 64\n",
      "Büşra Coşkuner : 350\n",
      "Daniel Graf : 2\n",
      "Daniel Grossenbacher : 6\n",
      "David Butler : 136\n",
      "Dr. Martin Feuz : 46\n",
      "Farhad Ahmadyar : 8\n",
      "Gerhard Wesp : 44\n",
      "Jonas Kamber : 350\n",
      "Joshua Steffen : 60\n",
      "Julien Silva : 48\n",
      "Kateryna Osadchuk : 136\n",
      "Ksenija Korolova : 102\n",
      "Laurent Decrue : 350\n",
      "Lisa Winter : 134\n",
      "Marc Hauser : 350\n",
      "Martin Nyffenegger : 14\n",
      "Michael Lanker : 88\n",
      "Michael Scheiwiller : 6\n",
      "Michael Wood : 350\n",
      "Oliver Ganz : 326\n",
      "Oliver Notz : 4\n",
      "Patrick Fischbacher : 34\n",
      "Philippe Theis : 44\n",
      "René Goebels : 36\n",
      "Reto Laemmler : 288\n",
      "Robin Setzer : 4\n",
      "Sabine Wildemann : 212\n",
      "Stefan Birrer : 24\n",
      "Theresa Engl : 110\n",
      "Thomas Veit : 22\n",
      "Tigran Arzumanov : 326\n",
      "Tilman Eberle : 86\n",
      "Tobias Clemens : 158\n",
      "Valentin Binnendijk : 124\n",
      "Yulia Matiash : 2\n",
      "Total posts after balanced capping: 5124\n",
      "Is High Performing\n",
      "0    2562\n",
      "1    2562\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset by removing outliers, capping posts per user, and ensuring 50/50 HIGH/LOW performer distribution\n",
    "\n",
    "# Load the dataset\n",
    "df_ml = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Remove extreme viral hits that distort the mean)\n",
    "df_ml = df_ml[df_ml['Relative Engagement'] <= MAX_RELATIVE_ENGAGEMENT]\n",
    "\n",
    "# Remove very short posts\n",
    "df_ml = df_ml[df_ml['Post Content Length'] >= MIN_POST_LENGTH]\n",
    "\n",
    "# Bin Semantic Alignment into quintiles\n",
    "#df_ml[\"Semantic Alignment\"] = pd.qcut(df_ml[\"Semantic Alignment\"], 5, labels=False)\n",
    "\n",
    "# Normalize count features by Post Content Length\n",
    "# df_ml['Hook Length'] = df_ml[\"Hook Length\"] / (df_ml[\"Post Content Length\"] + 1)\n",
    "# df_ml[\"Emoji Count\"] = df_ml[\"Emoji Count\"] / (df_ml[\"Post Content Length\"] + 1)\n",
    "# df_ml['Hashtag Count'] = df_ml[\"Hashtag Count\"] / (df_ml[\"Post Content Length\"] + 1)\n",
    "# df_ml['Linebreak Count'] = df_ml[\"Linebreak Count\"] / (df_ml[\"Post Content Length\"] + 1)\n",
    "# df_ml['Link Count'] = df_ml[\"Link Count\"] / (df_ml[\"Post Content Length\"] + 1)\n",
    "\n",
    "# Get balanced high/low performance capping per user\n",
    "balanced_dfs = []\n",
    "for user_id, user_data in df_ml.groupby('User ID'):\n",
    "    # Split into classes\n",
    "    high_perf = user_data[user_data['Is High Performing'] == 1]\n",
    "    low_perf = user_data[user_data['Is High Performing'] == 0]\n",
    "    \n",
    "    # Determine the budget for this user\n",
    "    # If they have 5 posts total, we can at most take 2 of each to keep it 1:1\n",
    "    user_total_available = len(user_data)\n",
    "    cap_for_this_user = min(user_total_available, MAX_POSTS_PER_USER)\n",
    "    \n",
    "    # To be perfectly balanced, we can't take more than what the smaller class has\n",
    "    # and we can't take more than half of the total cap.\n",
    "    max_possible_per_class = cap_for_this_user // 2\n",
    "    n_to_take = min(len(high_perf), len(low_perf), max_possible_per_class)\n",
    "    \n",
    "    # Only sample if we actually have posts to take\n",
    "    if n_to_take > 0:\n",
    "        sampled_high = high_perf.sample(n=n_to_take, random_state=42)\n",
    "        sampled_low  = low_perf.sample(n=n_to_take, random_state=42)\n",
    "\n",
    "        balanced_chunk = pd.concat([sampled_high, sampled_low])\n",
    "        balanced_dfs.append(balanced_chunk)\n",
    "\n",
    "        print(user_id, \":\", len(balanced_chunk))\n",
    "    else:\n",
    "        # If a user ONLY has high or ONLY has low, they are excluded \n",
    "        # to maintain the 50/50 integrity of your training set.\n",
    "        continue\n",
    "\n",
    "# Combine all users back into one master dataframe\n",
    "df_ml = pd.concat(balanced_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Total posts after balanced capping: {len(df_ml)}\")\n",
    "print(df_ml['Is High Performing'].value_counts())\n",
    "\n",
    "# Save the cleaned and balanced dataset\n",
    "df_ml.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c2c48",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
